# -*- coding: utf-8 -*-
"""Tarea 3 CE IABG - Big Data Aplicada.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyn87vFYMHDwe7C9WcsoHIWxbbzkMA8u

# Ejercicios Tarea 3 Curso CE IABG del módulo Big Data Aplicado
## Jaime Rabasco Ronda

# Paso 1. Instalación de Hadoop y preparación del entorno

####1.1 Descargamos Hadoop
"""

!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz

"""####1.2 Descompresión de la distribución de hadoop"""

!tar -xzf hadoop-3.3.1.tar.gz

"""####1.3 Copiar a /usr/local# """

#copy  hadoop file to user/local
!cp -r hadoop-3.3.1/ /usr/local/

"""####1.4 Configurar el Hadoop JAVA HOME
Hadoop requiere que se establezca la ruta de acceso a Java, ya sea como una variable de entorno o en el archivo de configuración de Hadoop.
"""

import os
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] = os.environ["PATH"] + ":" + "/usr/local/hadoop-3.3.1/bin"

"""#### 1.5 Comprobando Hadoop
Comprobamos que lo tenemos instalado y nos da información sobre las opciones y parámetros de ejecución que tiene hadoop
"""

!hadoop version

"""#Paso 2. Instalación de Flume

####2.1 Descargamos Flume
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# wget https://dlcdn.apache.org/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz
# tar xzf apache-flume-1.9.0-bin.tar.gz
# mv apache-flume-1.9.0-bin/ /usr/local/

"""####2.2 Variables de entorno"""

os.environ["FLUME_HOME"] = "/usr/local/apache-flume-1.9.0-bin"
os.environ["PATH"] = os.environ["PATH"] + ":" + "/usr/local/apache-flume-1.9.0-bin/bin"
os.environ["JAVA_OPTS"]= "-Xms400m -Xmx3000m -Dcom.sun.management.jmxremote"

"""####2.3 Comprobando Flume"""

!flume-ng version

"""#Paso 3. Ejercicio 1. Ingesta de datos vía Flume
## Spooldir

### 3.1 Creando directorios
"""

!mkdir -p logs
!mkdir -p copy-logs

"""## 3.2 Creamos el fichero de configuración del agente"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile monitor_flume.conf
# # Configuración del Agente para monitorización de un directorio
# 
# #Definición del agente: nombre y componentes
# agente.sources  = s1  
# agente.sinks    = k1  
# agente.channels = c1  
#    
# #Configuración de propiedades Source
# agente.sources.s1.type              = spooldir
# agente.sources.s1.spoolDir          = /content/logs
# agente.sources.s1.basenameHeader    = true
# agente.sources.s1.basenameHeaderKey = fileName
# 
# # Configuración de propiedades del canal
# agente.channels.c1.type    = memory
# #agente.channels.c1.type = file
# #agente.channels.c1.type = SPILLABLEMEMORY
# #agente.channels.c1.memoryCapacity = 10000
# #agente.channels.c1.overflowCapacity = 1000000
# #agente.channels.c1.byteCapacity = 800000
# #agente.channels.c1.checkpointDir = /content/flume/checkpoint
# #agente.channels.c1.dataDirs = /content/flume/data
# 
# # Configuración de propiedades del sink
# agente.sinks.k1.type = file_roll
# agente.sinks.k1.sink.directory = /content/copy-logs
# 
# #Vinculación de source y sink al canal creado
# agente.sources.s1.channels = c1
# agente.sinks.k1.channel    = c1

"""## 3.3 Descargamos el fichero csv de Kaggle"""

!wget https://raw.githubusercontent.com/jaimerabasco/CE_IABG_BigDataAplicado_Actividades/main/Actividad3/Height_of_Male_and_Female_by_Country_2022.csv

"""##3.4 Lanzamos el agente Flume"""

!flume-ng agent --conf ./ -f ./monitor_flume.conf -n agente -Dflume.root.logger=INFO -Xmx3000m

"""1. Copiar el fichero csv descargado utilizando el manejador de archivos de la izquierda a logs.
2. Comprobar el log de flume
3. Comprobar el directorio copy-logs.
4. Flume se queda en ejecución continua no permitiendo ejecutar más cosas, por lo tanto ir a Entorno de Ejecución -> Interrumpir Ejecución

# Paso 4. Ejercicio 2. Acceso a los datos usando Spark-Hive

####4.1 Instalación e Importación de Datos en Hive utilizando PySpark
"""

!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
import os
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"
import findspark
findspark.init()

"""#### 4.2 Leer un fichero con spark (necesario para poder leer los ficheros que vamos a cargar)"""

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Learning_Spark") \
    .getOrCreate()

sc = spark.sparkContext

"""## 5.3 Leer el CSV en un Dataframe 
---
Observa la ruta del fichero creado en la carpeta copy-logs por flume. Copia la ruta y pegala en la lectura de fuente de spark

"""

data = spark.read.csv('/content/copy-logs/1645882908305-1',inferSchema=True, header=True)

rddL=sc.textFile("/content/copy-logs/1645882908305-1")
rddL.collect()

from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)
ex1 = hiveCtx.read.csv("/content/copy-logs/1645882908305-1")
ex1.registerTempTable("Height_es")
results = hiveCtx.sql("SELECT * FROM Height_es").show()

"""## 5.4 Filtrado del archivo csv, quedándote únicamente con las columnas “Country”, "Male Height in Cm","Female Height in Cm" y guardarlo en otro fichero"""

dataF=data.select("Country_Name", "Male_Height_Cm","Female_Height_Cm")

dataF.write.csv("Heights_filtrados.csv")

rddL=sc.textFile("Heights_filtrados.csv")
rddL.collect()

"""## 5.5 Mostrando el nuevo fichero filtrado"""

from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)
ex1 = hiveCtx.read.csv("Heights_filtrados.csv")
ex1.registerTempTable("Height_es")
results = hiveCtx.sql("SELECT * FROM Height_es").show()

"""# Paso 6. Ejercicio 3 Limpieza de datos con PIG

## 6.1 Instalación de Pig
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# wget https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz
# tar -xzf pig-0.17.0.tar.gz
# cp -r pig-0.17.0/ /usr/local/

"""## 6.2 Variables de Entorno"""

os.environ["PIG_HOME"] = "/usr/local/pig-0.17.0"
os.environ["PATH"] = os.environ["PATH"] + ":" + "/usr/local/pig-0.17.0/bin"
os.environ["PIG_CLASSPATH"] = "/usr/local/hadoop-3.3.0/conf"

"""## 6.3 Comprobación de instalación correcta"""

!pig -h -version

"""## 6.4 Comprobamos el fichero creado en flume

Recuerda la ruta del fichero en copy-logs, como hicimos en hive-spark
"""

!cat /content/copy-logs/1645882908305-1 | head

"""## 6.5 Nos aseguramos recoger las columnas correctamente"""

import pandas as pd
df = pd.read_csv("/content/copy-logs/1645882908305-1")
df.head(5)

df.columns

"""## 6.6 Procesar los nulos

1. Comprobamos si hay nulos en nuestro fichero
"""

print(df.shape)
pd.isnull(df).any()

"""2. Hacemos un copia del fichero original para posteriormente añadir valores nulos"""

!cp /content/copy-logs/1645882908305-1 /content/copy-logs/1645882908305-1_nulos

"""3. Abrimos el fichero creado y probocamos valores nulos y comprobamos de nuevo"""

import pandas as pd
df = pd.read_csv("/content/copy-logs/1645882908305-1_nulos")
df.head(5)

"""4. Comprobamos que ahora si detecta valores nulos"""

print(df.shape)
pd.isnull(df).any()

"""5. Proceso Pig para sustituir los valores nulos en los campos concretos. Almacenamos la salida procesada en /content/nulos"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile procesar_nulos.pig 
# 
# height = load '/content/copy-logs/1645882908305-1_nulos' using PigStorage(',') as (
#     Rank:int, Country_Name:chararray, Male_Height_Cm:float, Female_Height_Cm:float, Male_Height_Ft:float,
#        Female_Height_Ft:float);
#     
# height_clean = foreach height generate Rank..Country_Name,
#   ((Male_Height_Cm IS  NULL) ? 0 : Male_Height_Cm),((Female_Height_Cm IS  NULL) ? 0 : Female_Height_Cm),
#   ((Male_Height_Ft IS  NULL) ? 0 : Male_Height_Ft) , Female_Height_Ft;
#   
# 
# STORE height_clean INTO 'nulos' using PigStorage(',');

!pig procesar_nulos.pig

"""6. Comprobación mediante Python/Pandas"""



df_clean = pd.read_csv("/content/nulos/part-m-00000", skiprows=1, header = None, names = df.columns)
print(df_clean.shape)
pd.isnull(df_clean).any()