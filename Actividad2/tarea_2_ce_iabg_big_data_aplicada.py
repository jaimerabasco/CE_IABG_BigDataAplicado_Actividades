# -*- coding: utf-8 -*-
"""Tarea 2 CE IABG - Big Data Aplicada

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MPGDcutrTx4LiEXk0iWR3ELFeyhfGQgV

# Ejercicios Tarea 2 Curso CE IABG del módulo Big Data Aplicado
## Jaime Rabasco Ronda

# Paso 1. Instalación de Hadoop y preparación del entorno

####1.1 Descargamos Hadoop
"""

!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz

"""####1.2 Descompresión de la distribución de hadoop"""

!tar -xzf hadoop-3.3.0.tar.gz

"""####1.3 Copiar a /usr/local# """

#copy  hadoop file to user/local
!cp -r hadoop-3.3.0/ /usr/local/

"""####1.4 Configurar el Hadoop JAVA HOME
Hadoop requiere que se establezca la ruta de acceso a Java, ya sea como una variable de entorno o en el archivo de configuración de Hadoop.
Buscamos cual es la dirección de Java en la máquina Google Colab
"""

!readlink -f /usr/bin/java | sed "s:bin/java::"

"""####1.5 Establecemos mediante código Python el valor de esta variable"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64/"

"""#### 1.6 Ejecutando Hadoop
Comprobamos que lo tenemos instalado y nos da información sobre las opciones y parámetros de ejecución que tiene hadoop
"""

#Running Hadoop
!/usr/local/hadoop-3.3.0/bin/hadoop

"""También podemos usar la opción -version para comprobar la correcta instalación y ejecución de hadoop"""

!/usr/local/hadoop-3.3.0/bin/hadoop version

"""#### 1.7 Observamos todas las aplicaciones que trae mapreduce"""

!ls -la /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/

"""#### 1.8 Investigando los ejemplos de Map Reduce. A nosotros nos interesa "hadoop-mapreduce-examples-3.3.0.jar" 
Miramos todas las opciones de aplicación de ejemplos de Map Reduce ejecutándolo sin parametros. Obtenemos la lista de ejemplos
"""

!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar

"""####1.9 Vemos que tenemos la posibilidad de jugar con dos ejemplos: pi y sudoku

#Paso 2.
###Ejemplo 1: Cálculo del número π, vía Hadoop Map/Reduce, con 16 maps y 10000000 muestras

#### 2.1 Primero ejecutamos pi sin parámetros para que nos de información sobre las opciones y/o parámetros para poder resolver el ejercicio propuesto
"""

!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi

"""#### 2.2 Vemos que podemos pasarle 2 parámetros:
*   nMaps: Número de mapas
*   nSamples: Número de muestras
Por tanto, aplicamos pi con 16 mapas y 10000000 muestras




"""

!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 16 10000000

"""#Paso 3.
###Ejemplo 2: Resolución, vía Hadoop Map/Reduce, del siguiente Sudoku.

#### 3.1 Primero ejecutamos sudoku sin parámetros para que nos de información sobre las opciones y/o parámetros para poder resolver el ejercicio propuesto
"""

!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar sudoku

"""####3.2 Nos indica que tenemos que pasar por linea de comandos el sudoku. Creamos un fichero con los datos del sudoku"""

!mkdir ~/ejemplo2

!echo "8 5 ? 3 9 ? ? ? ?" > ~/ejemplo2/sudoku1.data
!echo "? ? 2 ? ? ? ? ? ?" >> ~/ejemplo2/sudoku1.data
!echo "? ? 6 ? 1 ? ? ? 2" >> ~/ejemplo2/sudoku1.data
!echo "? ? 4 ? ? 3 ? 5 9" >> ~/ejemplo2/sudoku1.data
!echo "? ? 8 9 ? 1 4 ? ?" >> ~/ejemplo2/sudoku1.data
!echo "3 2 ? 4 ? ? 8 ? ?" >> ~/ejemplo2/sudoku1.data
!echo "9 ? ? ? 8 ? 5 ? ?" >> ~/ejemplo2/sudoku1.data
!echo "? ? ? ? ? ? 2 ? ?" >> ~/ejemplo2/sudoku1.data
!echo "? ? ? ? 4 5 ? 7 8" >> ~/ejemplo2/sudoku1.data

"""####3.3 Comprobamos que se ha creado correctamente"""

!cat ~/ejemplo2/sudoku1.data

"""####3.4 Una vez creado el fichero que contiene el sudoku ejecutamos para hallar la solución"""

!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar sudoku ~/ejemplo2/sudoku1.data

"""# Paso 4. Preparación del Entorno con Apache Spark

####4.1 Instalación e Importación de Datos en Hive utilizando PySpark
"""

!wget -q https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
!tar xf spark-3.2.0-bin-hadoop3.2.tgz
!pip install -q findspark
import os
os.environ["SPARK_HOME"] = "/content/spark-3.2.0-bin-hadoop3.2"
import findspark
findspark.init()

"""#### 4.2 Leer un fichero con spark (necesario para poder leer los ficheros que vamos a cargar)"""

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Learning_Spark") \
    .getOrCreate()

sc = spark.sparkContext

"""#Paso 5
## Ejercicio 3:
Tras descargar el fichero: [https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_ccaa.csv](https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_ccaa.csv) realiza:


* 3.1) Filtrado del archivo csv, quedándote únicamente con las columnas “ccaa_iso”,
“fecha” y “num_casos”
* 3.2) Finalizado el filtrado, almacena el nuevo contenido en otro fichero csv
* 3.3) Creación de una tabla en Apache Hive partiendo del archivo anterior.

Este tercer ejercicio se deberá realizar utilizando Apache Spark.

####5.1 Cargar el fichero del ejercicio desde colab [https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_ccaa.csv](https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_ccaa.csv)
"""

from google.colab import files
files.upload()

"""5.2 Leer el CSV en un Dataframe filtrarlo y salvarlo"""

data = spark.read.csv('casos_diagnostico_ccaa.csv',inferSchema=True, header=True)

"""####5.3 **Ejercicio 3.1) Filtrado del archivo csv, quedándote únicamente con las columnas “ccaa_iso”,“fecha” y “num_casos”**"""

dataF=data.select("ccaa_iso","fecha","num_casos")

"""####5.4 **Ejercicio 3.2) Guardar solo los datos filtrados**"""

dataF.write.csv("casos_diagnostico_ccaa_filtrados.csv")

"""####5.5 Leemos este nuevo archivo en un RDD"""

rddL=sc.textFile("casos_diagnostico_ccaa_filtrados.csv")
rddL.collect()

"""####5.6 **Ejercicio 3.3) Creación de una tabla en Apache Hive partiendo del archivo anterior**"""

from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)
ex1 = hiveCtx.read.csv("casos_diagnostico_ccaa_filtrados.csv")
ex1.registerTempTable("ccaa")
results = hiveCtx.sql("SELECT * FROM ccaa").show()